{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceType":"datasetVersion","sourceId":11184753,"datasetId":6981805,"databundleVersionId":11587190},{"sourceType":"datasetVersion","sourceId":11218058,"datasetId":7005518,"databundleVersionId":11626380},{"sourceType":"datasetVersion","sourceId":2624724,"datasetId":1595713,"databundleVersionId":2668532},{"sourceType":"datasetVersion","sourceId":11184771,"datasetId":6981820,"databundleVersionId":11587209}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import lr_scheduler\nimport re\nimport pandas as pd\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nimport random\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport torch.nn.functional as F\nfrom nltk.tokenize import RegexpTokenizer\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import f1_score, precision_score, recall_score\nimport math\n\n\n# Ensure necessary NLTK data is downloaded\nnltk.download('punkt')\nnltk.download('stopwords')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_csv_dataset(csv_path):\n    df = pd.read_csv(csv_path)\n    texts=[]\n    for i in range(df.shape[0]):\n        texts.append(df.iloc[i][\"Text\"].lower())  # Convert to lowercase\n    return texts\n    \n# def get_stopwords():\n#     with open('/kaggle/input/stopwords-txt/stopwords.txt') as f:\n#         stopwords = f.read().replace('\\n',' ').split()\n#     return stopwords\n\ndef clean_text(text):\n    tokenizer = RegexpTokenizer(r\"\\d|\\w+\")\n    words=tokenizer.tokenize(text)\n    # text = re.sub(r'[^a-zA-Z\\s]', '', text)  \n    # words = word_tokenize(text.lower()) \n    stop_words = set(stopwords.words('english')) \n    return [word for word in words if word not in stop_words]\n\n\ndef build_vocab(texts, vocab_size):\n    all_words=[]\n    for i in range(len(texts)):\n        word_list=clean_text(texts[i])\n        for word in word_list:\n            all_words.append(word)\n    # print(len(all_words))\n    word_counts = Counter(all_words)\n    for word in all_words:\n        word_counts.update([word])\n    most_common = word_counts.most_common(vocab_size - 1)\n    # print(len(most_common))\n    \n    word_to_idx = {}\n\n    for idx, (word, count) in enumerate(most_common):\n        word_to_idx[word] = idx+1\n\n    word_to_idx['<UNK>'] = 0 \n    idx_to_word = {}\n    for word, idx in word_to_idx.items():\n        idx_to_word[idx] = word\n    return word_to_idx, idx_to_word\n\ndef generate_cbows(texts,word_to_idx, window_size):\n    # training_data=[]\n    cbows = []\n    for text in texts:\n        tokenized_text=clean_text(text)\n        indices = [word_to_idx.get(word, 0) for word in tokenized_text]\n        for center_idx in range(window_size,len(indices)-window_size):\n                context_indices=[]\n                for w in range(-window_size, window_size + 1):\n                    if w == 0:\n                        continue\n                    context_indices.append(indices[center_idx + w])\n                cbows.append((context_indices,indices[center_idx]))\n                    # if (context_indices==2*window_size):\n\n    return cbows","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score\nimport torch.nn as nn\nimport torch\n\ndef train_model(model, train_dataloader, validation_dataloader, epochs, learning_rate):\n\n    loss_fn = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n\n    train_set_loss_log = []\n    validation_set_loss_log = []\n    train_accuracy_log = []\n    validation_accuracy_log = []\n    train_f1_log = []\n    validation_f1_log = []\n\n    for epoch in range(epochs):\n        print(f\"\\nEpoch: {epoch+1}/{epochs}\")\n\n        # ---- TRAINING PHASE ----\n        model.train()\n        total_train_loss = 0.0\n        num_train_batches = 0\n        all_train_preds = []\n        all_train_labels = []\n\n        for inputs_batch, outputs_batch in train_dataloader:\n            inputs_batch = inputs_batch.to(device)\n            outputs_batch = outputs_batch.to(device)\n\n            y_train_logits = model(inputs_batch)\n            train_loss = loss_fn(y_train_logits, outputs_batch)\n\n            optimizer.zero_grad()\n            train_loss.backward()\n            optimizer.step()\n\n            total_train_loss += train_loss.item()\n            num_train_batches += 1\n\n            preds = torch.argmax(y_train_logits, dim=1)\n            all_train_preds.extend(preds.cpu().numpy())\n            all_train_labels.extend(outputs_batch.cpu().numpy())\n\n        average_train_loss = total_train_loss / num_train_batches\n        train_set_loss_log.append(average_train_loss)\n\n        train_acc = accuracy_score(all_train_labels, all_train_preds)\n        train_f1 = f1_score(all_train_labels, all_train_preds, average='weighted')  # or 'macro'\n        train_accuracy_log.append(train_acc)\n        train_f1_log.append(train_f1)\n\n        # ---- VALIDATION PHASE ----\n        model.eval()\n        total_validation_loss = 0.0\n        num_validation_batches = 0\n        all_val_preds = []\n        all_val_labels = []\n\n        with torch.inference_mode():\n            for inputs_batch, outputs_batch in validation_dataloader:\n                inputs_batch = inputs_batch.to(device)\n                outputs_batch = outputs_batch.to(device)\n\n                y_val_logits = model(inputs_batch)\n                val_loss = loss_fn(y_val_logits, outputs_batch)\n\n                total_validation_loss += val_loss.item()\n                num_validation_batches += 1\n\n                preds = torch.argmax(y_val_logits, dim=1)\n                all_val_preds.extend(preds.cpu().numpy())\n                all_val_labels.extend(outputs_batch.cpu().numpy())\n\n        average_validation_loss = total_validation_loss / num_validation_batches\n        validation_set_loss_log.append(average_validation_loss)\n\n        val_acc = accuracy_score(all_val_labels, all_val_preds)\n        val_f1 = f1_score(all_val_labels, all_val_preds, average='weighted')  # or 'macro'\n        validation_accuracy_log.append(val_acc)\n        validation_f1_log.append(val_f1)\n\n        print(f\"Train Loss: {average_train_loss:.4f} | Val Loss: {average_validation_loss:.4f}\")\n        print(f\"Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n        print(f\"Train F1: {train_f1:.4f} | Val F1: {val_f1:.4f}\")\n\n    return model, {\n        'train_loss': train_set_loss_log,\n        'val_loss': validation_set_loss_log,\n        'train_acc': train_accuracy_log,\n        'val_acc': validation_accuracy_log,\n        'train_f1': train_f1_log,\n        'val_f1': validation_f1_log\n    }\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CbowWord2Vec(nn.Module):\n    def __init__(self, vocab_size, embedding_dim) -> None:\n        super().__init__()\n        self.embeddings = nn.Embedding(vocab_size,embedding_dim)  \n        self.linear = nn.Linear(embedding_dim,vocab_size)\n\n    def forward(self, X) -> torch.Tensor: \n        embeddings=self.embeddings(X).mean(1).squeeze(1)\n        embeddings=self.linear(embeddings)\n        return embeddings","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"######################################################\nbatch_size=64\nsplit_ratio=0.8\nvocab_size=10000\nwindow_size=2\n######################################################","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# cbow_vector_pairs[0]\n# cbow_vector_pairs[0][0].sum()\nclass CustomDataset(Dataset):\n    def __init__(self, data):\n        self.inputs = torch.tensor([item[0] for item in data])\n        self.outputs = torch.tensor([item[1] for item in data])\n        # print(self.outputs[0])\n\n    def __len__(self):\n        return len(self.inputs)\n\n\n    def __getitem__(self, idx):\n        input_sample = self.inputs[idx]\n        output_sample = self.outputs[idx]\n    \n        return input_sample, output_sample\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"csv_path = \"/kaggle/input/bbc-news-article/TrainData.csv\" \ntexts = load_csv_dataset(csv_path)\nword_to_idx,idx_to_word=build_vocab(texts,vocab_size)\ncbows=generate_cbows(texts,word_to_idx,window_size)\n\n\nrandom.shuffle(cbows)\n\nsplit_index = int(len(cbows) * split_ratio)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = CustomDataset(cbows[:split_index])\ntest_dataset = CustomDataset(cbows[split_index:])\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nvalidation_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"embedding_dim = 20\nepochs=3\nlearning_rate=0.01\n# vocab_size=10000","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = CbowWord2Vec(vocab_size, embedding_dim).to(device)\nmodel, model_dict = train_model(model, train_dataloader, validation_dataloader, \n                                                                 epochs, learning_rate)\ntrain_set_loss_log=model_dict['train_loss']\nvalidation_set_loss_log=model_dict['val_loss']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def cosine_similarity(v1, v2):\n\n    return (v1 @ v2) / (torch.norm(v1) * torch.norm(v2))\n\ndef most_similar(word, word_dict, top_k=5):\n        \n    query_vector = word_dict[word]\n\n    similarities = {}\n    for other_word, other_vector in word_dict.items():\n        if other_word != word:\n            similarity = cosine_similarity(query_vector, other_vector)\n            similarities[other_word] = similarity\n\n    sorted_similarities = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n    top_similar_words = sorted_similarities[:top_k]\n\n    return top_similar_words","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(train_set_loss_log, color='red', label='train_loss')\nplt.plot(validation_set_loss_log, color='blue', label='validation_loss')\n\nplt.title(\"Loss During Training\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Cross Entropy\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"params = list(model.parameters())\nword_vectors=model.embeddings.weight.detach().cpu()\nunique_words=[]\nfor i in range(vocab_size):\n    unique_words.append(idx_to_word[i])\n\nword_dict = {word: vector for word, vector in zip(unique_words, word_vectors)}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import kagglehub\npath = kagglehub.dataset_download(\"sugataghosh/google-word2vec\")\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from gensim.models import KeyedVectors\n\npath=\"/kaggle/input/google-word2vec/GoogleNews-vectors-negative300.bin\"\nmodel = KeyedVectors.load_word2vec_format(path, binary=True)\n\nprint(model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"unique_words = list(model.index_to_key)\nword_vectors = [model[word] for word in unique_words]\nword_dict = {word: vector for word, vector in zip(unique_words, word_vectors)}\nword_dict['<UNK>']=np.zeros(300)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(word_dict['police'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class NewsDataset(Dataset):\n    def __init__(self, csv_file, embedding_dict, max_len=100):\n        self.data = pd.read_csv(csv_file)\n        self.embedding_dict = embedding_dict\n        self.max_len = max_len\n        self.categories = {\"business\": 0, \"tech\": 1, \"politics\": 2, \"sport\": 3, \"entertainment\": 4}\n\n    def text_to_embedding(self, text):\n        words = clean_text(text)\n        embeddings=[]\n        for word in words[0:self.max_len]:\n            embeddings.append(self.embedding_dict.get(word, self.embedding_dict.get('<UNK>')))\n\n        temp=len(embeddings)\n        while temp < self.max_len:\n            embeddings.append(np.zeros(len(self.embedding_dict['<UNK>'])))\n            temp+=1\n\n        embeddings = np.array([e.cpu().numpy() if isinstance(e, torch.Tensor) else e for e in embeddings], dtype=np.float32)\n    \n        return embeddings\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        text_embedding = self.text_to_embedding(self.data.iloc[idx][\"Text\"])\n        label = self.categories[self.data.iloc[idx][\"Category\"]]\n        return torch.tensor(text_embedding, dtype=torch.float32).cpu(), torch.tensor(label, dtype=torch.long).cpu()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import random_split\nmaxlen=100\nfull_dataset = NewsDataset(csv_path, word_dict, maxlen)\ntrain_size = int(0.8 * len(full_dataset))\nval_size = len(full_dataset) - train_size\ntrain_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset[0][0][-1]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_model(model, dataloader, device, average='weighted'):\n    model.eval()\n    total_correct = 0\n    total_samples = 0\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for inputs, labels in dataloader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            preds = outputs.argmax(dim=1)\n\n            total_correct += (preds == labels).sum().item()\n            total_samples += labels.size(0)\n\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    accuracy = total_correct / total_samples\n    f1 = f1_score(all_labels, all_preds, average=average)\n    precision = precision_score(all_labels, all_preds, average=average)\n    recall = recall_score(all_labels, all_preds, average=average)\n\n    results = {\n        'accuracy': accuracy,\n        'f1_score': f1,\n        'precision': precision,\n        'recall': recall,\n        'correct': total_correct,\n        'incorrect': total_samples - total_correct\n    }\n\n\n    print(f\"Test Accuracy : {results['accuracy']:.4f}\")\n    print(f\"F1 Score      : {results['f1_score']:.4f}\")\n    print(f\"Precision     : {results['precision']:.4f}\")\n    print(f\"Recall        : {results['recall']:.4f}\")\n    print(f\"Correct       : {results['correct']}\")\n    print(f\"Incorrect     : {results['incorrect']}\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CLSTM_B(nn.Module):\n    def __init__(self, embed_dim, hidden_dim, num_classes, kernel_sizes=[3,5,7], num_filters=100, dropout=0.1,max_len=100,use_self_attention=True):\n        super().__init__()\n\n        self.conv1=nn.Conv1d(in_channels=embed_dim, out_channels=num_filters, kernel_size=kernel_sizes[0], padding=1)\n        self.conv2=nn.Conv1d(in_channels=embed_dim, out_channels=num_filters, kernel_size=kernel_sizes[1],padding=2) \n        self.conv3=nn.Conv1d(in_channels=embed_dim, out_channels=num_filters, kernel_size=kernel_sizes[2],padding=3)\n        self.lstm_hidden_dim=hidden_dim\n\n        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n\n        self.fc1 = nn.Linear(num_filters*3+hidden_dim,num_classes*3)\n        self.fc2 = nn.Linear(num_classes*3,num_classes)\n        self.use_self_attention=use_self_attention\n        self.lstm_cell = nn.LSTMCell(input_size=embedding_dim, hidden_size=hidden_dim)\n        \n    def forward(self, x):\n \n        x_cnn = x.permute(0, 2, 1)  # (batch_size, embed_dim, seq_length)\n        x_cnn1=F.relu(self.conv1(x_cnn)).permute(0,2,1)\n        x_cnn2=F.relu(self.conv2(x_cnn)).permute(0,2,1)\n        x_cnn3=F.relu(self.conv3(x_cnn)).permute(0,2,1)\n\n        x_final=[x_cnn1,x_cnn2,x_cnn3]\n        x_final_cnn=torch.cat(x_final,dim=2)\n\n        x_final_cnn=torch.mean(x_final_cnn,dim=1)\n########################################################################################################\n\n        if self.use_self_attention:\n            \n            batch_size,seq_len,_ = x.size()\n            h_t = torch.zeros(batch_size, self.lstm_hidden_dim, device=x.device)\n            c_t = torch.zeros(batch_size, self.lstm_hidden_dim, device=x.device)\n            \n            hidden_states = []\n            \n            for t in range(seq_len):\n                current_input = x[:, t, :]  # (batch, num_filters)\n                h_t, c_t = self.lstm_cell(current_input, (h_t, c_t))\n                if t > 0:\n                    prev_h = torch.stack(hidden_states, dim=1)\n                    attn_scores = torch.bmm(prev_h, h_t.unsqueeze(2)).squeeze(2)\n                    attn_weights = F.softmax(attn_scores, dim=1)  # (batch, t, 1)\n                    attn_vector = torch.sum(prev_h * attn_weights.unsqueeze(2), dim=1)\n\n                    h_t = h_t + attn_vector\n                hidden_states.append(h_t)\n        \n            h_lstm = hidden_states[-1]\n        \n########################################################################################################\n        \n        else:\n            _, (h_lstm, _) = self.lstm(x)  # h_lstm: (1, batch_size, hidden_dim)\n            h_lstm=h_lstm.squeeze(0)\n    \n        x_final=[x_final_cnn,h_lstm]\n        x_final=torch.cat((x_final_cnn,h_lstm),dim=1)\n    \n        x_final=self.fc1(x_final)\n        x_final=self.fc2(x_final)\n        return x_final\n        \n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# vocab_size = 20000      \nembedding_dim = 300 #20   \nnum_filters = 50      \nlstm_hidden_dim = 64\nnum_classes = 5        \nmax_len = 100          \n# dropout_rate = 0.2\nuse_self_attention=True\nlearning_rate=0.001\nnum_epochs = 10\nkernel_sizes=[3,5,7]\nmodel=CLSTM_B(embedding_dim,lstm_hidden_dim, num_classes,kernel_sizes, num_filters,use_self_attention=use_self_attention)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n# optimizer = torch.optim.SGD(params=model.parameters(), lr=learning_rate, momentum=0.9)\n\ntrain_loss_log=[]\nval_loss_log=[]\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss, train_correct, train_total = 0.0, 0, 0\n    \n    for inputs, labels in train_dataloader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        train_loss += loss.item() * inputs.size(0)\n        train_correct += (outputs.argmax(1) == labels).sum().item()\n        train_total += labels.size(0)\n    train_loss_log.append(train_loss/train_total)\n    val_loss, val_correct, val_total = 0.0, 0, 0\n    model.eval()\n    with torch.no_grad():\n        for inputs, labels in val_dataloader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item() * inputs.size(0)\n            val_correct += (outputs.argmax(1) == labels).sum().item()\n            val_total += labels.size(0)\n    val_loss_log.append(val_loss/val_total)\n    \n    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss/train_total:.4f}, Train Acc: {train_correct/train_total:.2f}, Val Loss: {val_loss/val_total:.4f},Val Acc: {val_correct/val_total:.2f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(train_loss_log, color='red', label='train_loss')\nplt.plot(val_loss_log, color='blue', label='validation_loss')\n\nplt.title(\"Loss During Training\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Cross Entropy\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_csv_path=\"/kaggle/input/bbc-news-test-final/TestLabels.csv\"\ntest_dataset = NewsDataset(test_csv_path, word_dict, maxlen)\nbatch_size=64\ntest_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\nevaluate_model(model,test_dataloader,device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CLSTM_A(nn.Module):\n    def __init__(self, embedding_dim, num_filters, filter_size, \n                 lstm_hidden_dim, num_classes, use_self_attention=False,dropout_rate=0.2):\n        super().__init__()\n        self.conv = nn.Conv1d(in_channels=embedding_dim,\n                              out_channels=num_filters,\n                              kernel_size=filter_size)\n        self.dropout = nn.Dropout(dropout_rate)\n        self.lstm = nn.LSTM(input_size=num_filters, \n                            hidden_size=lstm_hidden_dim, \n                            batch_first=True)\n        self.lstm_cell = nn.LSTMCell(input_size=num_filters, hidden_size=lstm_hidden_dim)\n        self.fc = nn.Linear(lstm_hidden_dim, num_classes)\n        self.lstm_hidden_dim = lstm_hidden_dim\n        self.use_self_attention=use_self_attention\n\n    def forward(self, x):\n        x = self.dropout(x)\n        x = x.transpose(1, 2)  # (batch, embedding_dim, max_len)\n        conv_out = F.relu(self.conv(x))  \n        #  (batch, num_filters, L_out) \n        conv_out = conv_out.transpose(1, 2)   #(batch, L_out, num_filters)\n        \n        if self.use_self_attention:\n            \n            batch_size,seq_len,_ = conv_out.size()\n\n            h_t = torch.zeros(batch_size, self.lstm_hidden_dim, device=conv_out.device)\n            c_t = torch.zeros(batch_size, self.lstm_hidden_dim, device=conv_out.device)\n            \n            hidden_states = []\n            for t in range(seq_len):\n                current_input = conv_out[:, t, :]  # (batch, num_filters)\n                h_t, c_t = self.lstm_cell(current_input, (h_t, c_t))\n\n                if t > 0:\n                    prev_h = torch.stack(hidden_states, dim=1)\n                    attn_scores = torch.bmm(prev_h, h_t.unsqueeze(2)).squeeze(2)\n                    attn_weights = F.softmax(attn_scores, dim=1)  # (batch, t, 1)\n                    attn_vector = torch.sum(prev_h * attn_weights.unsqueeze(2), dim=1)\n\n                    h_t = h_t + attn_vector\n                hidden_states.append(h_t)\n\n            final_feature = self.dropout(h_t)\n        else:\n            lstm_out, (h_n, c_n) = self.lstm(conv_out)\n            final_feature = h_n.squeeze(0)  # (batch, lstm_hidden_dim)\n            final_feature = self.dropout(final_feature)\n            \n        logits = self.fc(final_feature)\n        return logits","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example hyperparameters\n# vocab_size = 20000      \n# embedding_dim = 20    \nnum_filters = 50       \nfilter_size = 10         \nlstm_hidden_dim = 64\nnum_classes = 5         \nmax_len = 100            \ndropout_rate = 0.2\nuse_self_attention=True\nlearning_rate=0.0001\nnum_epochs = 30 \nmodel=CLSTM_A(embedding_dim, num_filters, filter_size, \n                 lstm_hidden_dim, num_classes, use_self_attention,dropout_rate=dropout_rate)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\ncriterion = nn.CrossEntropyLoss()\n# optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n# optimizer = torch.optim.SGD(params=model.parameters(), lr=learning_rate, momentum=0.9)\noptimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n\ntrain_loss_log=[]\nval_loss_log=[]\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss, train_correct, train_total = 0.0, 0, 0\n    \n    for inputs, labels in train_dataloader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        train_loss += loss.item() * inputs.size(0)\n        train_correct += (outputs.argmax(1) == labels).sum().item()\n        train_total += labels.size(0)\n\n    train_loss_log.append(train_loss/train_total)\n    \n    val_loss, val_correct, val_total = 0.0, 0, 0\n    model.eval()\n    with torch.no_grad():\n        for inputs, labels in val_dataloader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item() * inputs.size(0)\n            val_correct += (outputs.argmax(1) == labels).sum().item()\n            val_total += labels.size(0)\n\n    val_loss_log.append(val_loss/val_total)\n    \n    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss/train_total:.4f}, Train Acc: {train_correct/train_total:.2f}, Val Loss: {val_loss/val_total:.4f},Val Acc: {val_correct/val_total:.2f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(train_loss_log, color='red', label='train_loss')\nplt.plot(val_loss_log, color='blue', label='validation_loss')\n\nplt.title(\"Loss During Training\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Cross Entropy\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_csv_path=\"/kaggle/input/bbc-news-test-final/TestLabels.csv\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_dataset = NewsDataset(test_csv_path, word_dict, maxlen)\nbatch_size=64\ntest_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\nevaluate_model(model,test_dataloader,device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TransformerEncoderBlock(nn.Module):\n    def __init__(self, embed_dim, num_heads, ff_hidden_dim, dropout=0.1):\n        super().__init__()\n        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n        self.fc1 = nn.Linear(embed_dim, ff_hidden_dim)\n        self.fc2 = nn.Linear(ff_hidden_dim, embed_dim)\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.norm2 = nn.LayerNorm(embed_dim)\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n    \n    def forward(self, x, mask=None):\n        attn_output, _ = self.self_attn(x, x, x, attn_mask=mask)\n        x = x + self.dropout1(attn_output)\n        x = self.norm1(x)\n        x1=self.fc1(x)\n        x1=F.relu(x1)\n        x1=self.fc2(x1)\n        x = x + self.dropout2(x1)\n        x = self.norm2(x)\n        return x\n\nclass TransformerTextEncoder(nn.Module):\n    def __init__(self, embed_dim, num_heads, ff_hidden_dim, num_layers, num_classes, use_positional_encoding=True, dropout=0.1):\n        super().__init__()\n        #######################################################\n        self.pe = torch.zeros(100, embed_dim)\n        position = torch.arange(0, 100, dtype=torch.float).unsqueeze(1)\n        self.div_term = torch.arange(0, embed_dim, 2).float()\n        self.div_term = self.div_term * (-math.log(10000.0) / embed_dim)\n        self.div_term = torch.exp(self.div_term)\n        self.pe[:, 0::2] = torch.sin(position * self.div_term)\n        self.pe[:, 1::2] = torch.cos(position * self.div_term)\n        self.pe = self.pe.unsqueeze(0)\n        ###################################################\n        self.layers=[]\n        for i in range(num_layers):\n            self.layers.append(TransformerEncoderBlock(embed_dim, num_heads, ff_hidden_dim, dropout))\n        self.transformer_layers=nn.Sequential(*self.layers)\n        self.norm = nn.LayerNorm(embed_dim)\n        self.fc = nn.Linear(embed_dim, num_classes)\n        self.use_positional_encoding=use_positional_encoding\n    \n    def forward(self, x, mask=None):\n        x=x\n        if self.use_positional_encoding:\n            x = x + self.pe[:, :x.size(1), :].to(x.device)\n        for layer in self.transformer_layers:\n            x = layer(x, mask)\n        x = self.norm(x.mean(dim=1)) \n        return self.fc(x)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_epochs = 20\nlearning_rate = 0.0001\nnum_of_heads = 5\nlayers = [2,4,6]\nff_dim = 2*embedding_dim\nuse_positional_encoding_list = [False,True]\ndropout=0.2\n\n\ntest_csv_path = \"/kaggle/input/bbc-news-test-final/TestLabels.csv\"\ntest_dataset = NewsDataset(test_csv_path, word_dict, maxlen)\nbatch_size = 64\ntest_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\nfor use_positional_encoding in use_positional_encoding_list:\n    for num_of_layers in layers:\n        \n        print('use_positional_encoding',use_positional_encoding,';  num_of_layers',num_of_layers)\n        \n        model = TransformerTextEncoder(\n            embed_dim=embedding_dim,\n            num_heads=num_of_heads,\n            ff_hidden_dim=ff_dim,\n            num_layers=num_of_layers,\n            num_classes=5,\n            dropout=0.1,\n            use_positional_encoding=use_positional_encoding\n        ).to(device)\n        \n        criterion = nn.CrossEntropyLoss()\n        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n        \n        for epoch in range(num_epochs):\n            model.train()\n            train_loss, train_correct, train_total = 0.0, 0, 0\n            all_train_preds, all_train_labels = [], []\n        \n            for inputs, labels in train_dataloader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                optimizer.zero_grad()\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                loss.backward()\n                optimizer.step()\n        \n                train_loss += loss.item() * inputs.size(0)\n                preds = outputs.argmax(dim=1)\n                train_correct += (preds == labels).sum().item()\n                train_total += labels.size(0)\n        \n                all_train_preds.extend(preds.cpu().numpy())\n                all_train_labels.extend(labels.cpu().numpy())\n        \n            train_f1 = f1_score(all_train_labels, all_train_preds, average='weighted')\n        \n            model.eval()\n            val_loss, val_correct, val_total = 0.0, 0, 0\n            all_val_preds, all_val_labels = [], []\n        \n            with torch.no_grad():\n                for inputs, labels in val_dataloader:\n                    inputs, labels = inputs.to(device), labels.to(device)\n                    outputs = model(inputs)\n                    loss = criterion(outputs, labels)\n        \n                    val_loss += loss.item() * inputs.size(0)\n                    preds = outputs.argmax(dim=1)\n                    val_correct += (preds == labels).sum().item()\n                    val_total += labels.size(0)\n        \n                    all_val_preds.extend(preds.cpu().numpy())\n                    all_val_labels.extend(labels.cpu().numpy())\n        \n            val_f1 = f1_score(all_val_labels, all_val_preds, average='weighted')\n\n        evaluate_model(model,test_dataloader,device)\n        \n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}